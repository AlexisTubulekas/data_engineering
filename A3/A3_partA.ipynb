{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "industrial-chicago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conscious-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.113:7077\") \\\n",
    "        .appName(\"alexistubulekasA3_partA\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .config(\"spark.cores.max\",2)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imposed-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sv = spark_session.sparkContext.textFile('hdfs://192.168.2.113:9000/europarl/europarl-v7.sv-en.sv').cache()\n",
    "#en = spark_session.sparkContext.textFile('hdfs://192.168.2.113:9000/europarl/europarl-v7.sv-en.en').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "trying-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# New API        \n",
    "#\n",
    "#spark_session = SparkSession\\\n",
    "#        .builder\\\n",
    "#        .master(\"spark://192.168.2.113:7077\")  \\\n",
    "#        .appName(\"alexistubulekasA3_partA\")\\\n",
    "#        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "#        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "#        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "#        .config(\"spark.executor.cores\",2)\\\n",
    "#        .config(\"spark.cores.max\",2)\\\n",
    "#        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "promotional-dimension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1862234"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_en = spark_context.newAPIHadoopFile(\n",
    "    'hdfs://192.168.2.113:9000/europarl/europarl-v7.sv-en.en',\n",
    "    'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'org.apache.hadoop.io.Text',\n",
    "    conf={'textinputformat.record.delimiter': '\\n'}\n",
    ")\\\n",
    ".cache() # Keep this RDD in memory!\n",
    "\n",
    "#A1.1\n",
    "rdd_en.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "veterinary-generation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A1.4\n",
    "rdd_en.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brilliant-bhutan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1862234"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sv = spark_context.newAPIHadoopFile(\n",
    "    'hdfs://192.168.2.113:9000/europarl/europarl-v7.sv-en.sv',\n",
    "    'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'org.apache.hadoop.io.Text',\n",
    "    conf={'textinputformat.record.delimiter': '\\n'}\n",
    ")\\\n",
    ".cache() # Keep this RDD in memory!\n",
    "\n",
    "#A1.2\n",
    "rdd_sv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "differential-armor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A1.4\n",
    "rdd_sv.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accompanied-sauce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'Resumption of the session'), (26, 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.'), (234, \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\")]\n"
     ]
    }
   ],
   "source": [
    "rdd_en_take3 = rdd_en.take(3)\n",
    "print(rdd_en_take3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lovely-winner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['resumption', 'of', 'the', 'session'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999,', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period.'], ['although,', 'as', 'you', 'will', 'have', 'seen,', 'the', 'dreaded', \"'millennium\", \"bug'\", 'failed', 'to', 'materialise,', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful.']]\n",
      "[['återupptagande', 'av', 'sessionen'], ['jag', 'förklarar', 'europaparlamentets', 'session', 'återupptagen', 'efter', 'avbrottet', 'den', '17', 'december.', 'jag', 'vill', 'på', 'nytt', 'önska', 'er', 'ett', 'gott', 'nytt', 'år', 'och', 'jag', 'hoppas', 'att', 'ni', 'haft', 'en', 'trevlig', 'semester.'], ['som', 'ni', 'kunnat', 'konstatera', 'ägde', '\"den', 'stora', 'år', '2000-buggen\"', 'aldrig', 'rum.', 'däremot', 'har', 'invånarna', 'i', 'ett', 'antal', 'av', 'våra', 'medlemsländer', 'drabbats', 'av', 'naturkatastrofer', 'som', 'verkligen', 'varit', 'förskräckliga.']]\n"
     ]
    }
   ],
   "source": [
    "#A2.1\n",
    "def lowercase_and_split(rdd_input):\n",
    "    \"\"\"Takes in a rdd and outputs all the values in second position in the tuple\n",
    "    lowercased and splited on space\"\"\"\n",
    "    return rdd_input.map(lambda w: w[1].lower().split(' '))\n",
    "\n",
    "# A.2.2 Inspect 10 entries from each of your RDDs to verify your pre-processing.\n",
    "rdd_en_lower_split = lowercase_and_split(rdd_en)\n",
    "rdd_sv_lower_split = lowercase_and_split(rdd_sv)\n",
    "\n",
    "print(rdd_en_lower_split.take(3))\n",
    "print(rdd_sv_lower_split.take(3))\n",
    "\n",
    "# A.2.3 Verify that the line counts still match after the pre-processing\n",
    "#print(rdd_en_lower_split.count())\n",
    "#print(rdd_sv_lower_split.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "digital-divide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3498375), ('of', 1659758), ('to', 1539760), ('and', 1288401), ('in', 1085993), ('that', 797516), ('a', 773522), ('is', 758050), ('for', 534242), ('we', 522849)]\n",
      "[('att', 1706293), ('och', 1344830), ('i', 1050774), ('det', 924866), ('som', 913276), ('för', 908680), ('av', 738068), ('är', 694381), ('en', 620310), ('vi', 539797)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A.3.1 Use Spark to compute the 10 most frequently according words in the English language corpus. Repeat for the other language.\n",
    "# A.3.2 Verify that your results are reasonable.\n",
    "from operator import add\n",
    "\n",
    "#English text\n",
    "all_words_en = rdd_en_lower_split.flatMap(lambda w: w)\\\n",
    "    .map(lambda w: (w,1))\n",
    "\n",
    "word_counts_en = all_words_en.reduceByKey(add)\n",
    "print(word_counts_en.takeOrdered(10, key=lambda x: -x[1]))\n",
    "\n",
    "#Swedish text\n",
    "all_words_sv = rdd_sv_lower_split.flatMap(lambda w: w)\\\n",
    "    .map(lambda w: (w,1))\n",
    "\n",
    "word_counts_sv = all_words_sv.reduceByKey(add)\n",
    "print(word_counts_sv.takeOrdered(10, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.4.1 \n",
    "# Use this parallel corpus to mine some translations in the form of word pairs, for the two languages. Do this by pairing words found on short lines with \n",
    "# the same number of words respectively. We (incorrectly) assume the words stay in the same order when translated.\n",
    "\n",
    "#Follow this approach. Work with the pair of RDDs you created in question A.2. Hint: make a new pair of RDDs for each step, sv_1, en_1, sv_2, en_2, ...\n",
    "\n",
    "# 1. Key the lines by their line number (hint: ZipWithIndex()).\n",
    "# 2. Swap the key and value - so that the line number is the key.\n",
    "# 3. Join the two RDDs together according to the line number key, so you have pairs of matching lines.\n",
    "# 4. Filter to exclude line pairs that have an empty/missing “corresponding” sentence.\n",
    "# 5. Filter to leave only pairs of sentences with a small number of words per sentence, this should give a more reliable translation (you can experiment).\n",
    "# 6. Filter to leave only pairs of sentences with the same number of words in each sentence.\n",
    "# 7. For each sentence pair, map so that you pair each (in order) word in the two sentences. We no longer need the line numbers. \n",
    "# hint: use python’s built in zip() function\n",
    "# 8. Use reduce to count the number of occurrences of the word-translation-pairs.\n",
    "# 9. Print some of the most frequently occurring pairs of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cleared-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Key the lines by their line number\n",
    "en_1 = rdd_en_lower_split\n",
    "en_1 = en_1.zipWithIndex()\n",
    "\n",
    "sv_1 = rdd_sv_lower_split\n",
    "sv_1 = sv_1.zipWithIndex()\n",
    "#print(en_1.take(2))\n",
    "#print(sv_1.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "social-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 2, 10)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "straight-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Swap the key and value - so that the line number is the key\n",
    "en_2 = en_1.map(lambda w: (w[1],w[0]))\n",
    "sv_2 = sv_1.map(lambda w: (w[1],w[0]))\n",
    "\n",
    "\n",
    "#Creating sublists to make computation easier\n",
    "list_range = range(5)\n",
    "en_2_sub = en_2.filter(lambda w:w[0] in list_range)\n",
    "sv_2_sub = sv_2.filter(lambda w:w[0] in list_range)\n",
    "\n",
    "join_sub = en_2_sub.join(sv_2_sub)\n",
    "join_sub = join_sub.filter(lambda w:w[1][0]!=[])\\\n",
    "                   .filter(lambda w:w[1][1]!=[])\n",
    "\n",
    "#join_sub.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-teaching",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "attempted-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Join on key\n",
    "joined_en_sv_3 = en_2.join(sv_2)\n",
    "#joined_en_sv.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "harmful-progressive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1235,\n",
       "  (['i',\n",
       "    'share',\n",
       "    'that',\n",
       "    'concern',\n",
       "    'and',\n",
       "    'i',\n",
       "    'believe',\n",
       "    'that',\n",
       "    'this',\n",
       "    'issue',\n",
       "    'should',\n",
       "    'be',\n",
       "    'addressed.'],\n",
       "   ['jag',\n",
       "    'delar',\n",
       "    'hennes',\n",
       "    'oro',\n",
       "    'och',\n",
       "    'anser',\n",
       "    'att',\n",
       "    'den',\n",
       "    'saken',\n",
       "    'bör',\n",
       "    'undersökas.']))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_en_sv_3.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ongoing-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Filter to exclude line pairs that have an empty/missing “corresponding” sentence.\n",
    "joined_en_sv_4 = joined_en_sv_3.filter(lambda w:w[1][0]!=[''])\\\n",
    "                   .filter(lambda w:w[1][1]!=[''])\n",
    "\n",
    "#joined_en_sv_4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opened-macro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(335995, (['protectionism.'], ['protektionism.'])),\n",
       " (425845, (['\\xa0\\xa0', '.'], ['\\xa0\\xa0', '.']))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. Filter to have only pairs of sentences with a small number of words per sentence (less than 4)\n",
    "joined_en_sv_5 = joined_en_sv_4.filter(lambda w:len(w[1][0])<4)\\\n",
    "                               .filter(lambda w:len(w[1][1])<4)\n",
    "\n",
    "joined_en_sv_5.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spread-warrior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(452600, (['\\xa0\\xa0', '.'], ['\\xa0\\xa0', '.'])),\n",
       " (533995, (['\\xa0\\xa0', '.'], ['\\xa0\\xa0', '.']))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6. Filter to leave only pairs of sentences with the same number of words in each sentence\n",
    "joined_en_sv_6 = joined_en_sv_5.filter(lambda w:len(w[1][0])==len(w[1][1]))\n",
    "                               \n",
    "\n",
    "joined_en_sv_6.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "passive-while",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('vote', 'omröstning'),),\n",
       " (('this', 'detta'), ('includes', 'inbegriper'), ('turkey.', 'turkiet.')),\n",
       " (('the', 'den'), ('second', 'andra'), ('occupation.', 'ockupationen.')),\n",
       " (('this', 'det'), ('is', 'är'), ('nonsense.', 'struntprat.'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7 For each sentence pair, map so that you pair each (in order) word in the two sentences. We no longer need the line numbers. (hint: use python’s built in zip() function)\n",
    "\n",
    "joined_en_sv_7 = joined_en_sv_6.map(lambda w:w[1])\\\n",
    "                               .map(lambda w:zip(w[0],w[1]))\\\n",
    "                               .map(lambda w:tuple(w))\n",
    "joined_en_sv_7.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hydraulic-novelty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('(applause)', '(applåder)'), 2546), (('.', '.'), 2081), (('\\xa0\\xa0', '\\xa0\\xa0'), 815), (('is', 'är'), 792), (('applause', 'applåder'), 451), (('1.', '1.'), 438), (('2.', '2.'), 438), (('3.', '3.'), 405), (('why?', 'varför?'), 369), (('that', 'det'), 291)]\n"
     ]
    }
   ],
   "source": [
    "# 8. Use reduce to count the number of occurrences of the word-translation-pairs\n",
    "from operator import add\n",
    "\n",
    "joined_en_sv_8 = joined_en_sv_7.flatMap(lambda w: (w))\\\n",
    "                               .map(lambda w: (w,1))\n",
    "#joined_en_sv_8.take(25)\n",
    "\n",
    "word_counts_joined_en_sv_8 = joined_en_sv_8.reduceByKey(add)\n",
    "print(word_counts_joined_en_sv_8.takeOrdered(10, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-might",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-constitution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "touched-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cores for another application!\n",
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-peeing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
